{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4 DL: Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab4: Segmentation\n",
        "\n",
        "## Introduction\n",
        "Three levels of image analysis:\n",
        "\n",
        "* **Classification:** Given an image $x$ and return as output a class $y = f_\\theta(x)$.\n",
        "* **Object detection:** Given an image $x$, identify meaningful object inside of it, draw a rectangle that specify its position and classify each indetified object.\n",
        "* **Segmentation:** Given an image $x$, classify each pixel of $x$ into one possible class.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is generally done with a Convolutional Neural Network that act as an image-to-image transform, mapping each pixel of $x$ to the corresponding class.\n",
        "\n",
        "*Remind:* Given an image $x \\in \\mathbb{R}^{m \\times n \\times c}$, an image-to-image map is a function $f: \\mathbb{R}^{m \\times n \\times c} \\to \\mathbb{R}^{m' \\times n' \\times c'}$. In our situation, $m = m'$, $n = n'$ and $c = c'$. An image-to-image map is required to do segmentation and some image processing tasks, but not for classification or object detection. \\\\\n",
        "\n",
        "Image-to-image maps are usually implemented by some variant of a Fully Convolutional Neural Network (FNN) design (e.g. ResNet, Autoencoders, ...). See https://heartbeat.comet.ml/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc for details.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "85iOmo8hW7oL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: Carvana Image Masking Challenge\n",
        "\n",
        "In our experiments today, we will use a famous dataset for image segmentation, which is called Carvana (https://www.kaggle.com/competitions/carvana-image-masking-challenge/overview). We need to download it:\n",
        "\n",
        "* Login with you kaggle account on https://www.kaggle.com/ .\n",
        "* Go to the link of the challenge and subscribe to the rules of the challenge.\n",
        "* In Account, press Create New API Token. This will download a file named $\\texttt{kaggle.json}$. Upload this file on Colab.\n",
        "* Run the following."
      ],
      "metadata": {
        "id": "_RquUSANZnwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su93JEBmTcoV"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle library\n",
        "!pip install kaggle\n",
        "\n",
        "# Create a new folder .kaggle and move kaggle.json into that\n",
        "!mkdir ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# Allocate permissions for this file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data from the competition. If an error occours, either you did\n",
        "# something wrong on the above or you forgot to subscribe the challenge.\n",
        "!kaggle competitions download -c carvana-image-masking-challenge"
      ],
      "metadata": {
        "id": "QI7owJ0Ubhqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is huge. For our experiments today, we just need a subset of it.\n",
        "# Unzip train.zip and the corresponding masks. Not the HQ version.\n",
        "!unzip -p carvana-image-masking-challenge.zip train.zip >train.zip\n",
        "!unzip -p carvana-image-masking-challenge.zip train_masks.zip >train_masks.zip\n",
        "\n",
        "# Unzip the zipped train files.\n",
        "!unzip train.zip\n",
        "!unzip train_masks.zip"
      ],
      "metadata": {
        "id": "ckufFE64eXNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Import libraries and prepare the code."
      ],
      "metadata": {
        "id": "kHS3sxW9f62B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "import os\n",
        "\n",
        "# Algebra\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Neural Networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as ks"
      ],
      "metadata": {
        "id": "SeKU6a7NgBVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_names(root_dir: str, fnames: list, shape=(256, 256)) -> np.array:\n",
        "    # Given the root path and a list of file names as input, return the dataset \n",
        "    # array.\n",
        "    images = []\n",
        "    \n",
        "    for idx, img_name in enumerate(fnames):\n",
        "        x = Image.open(os.path.join(root_dir, img_name))\n",
        "        x = x.resize(shape)\n",
        "        x = np.array(x)\n",
        "        images.append(x)\n",
        "\n",
        "        if idx%100 == 99:\n",
        "            print(f\"Processed {idx+1} images.\")\n",
        "    return np.array(images)\n",
        "\n",
        "# Load the names\n",
        "image_names = os.listdir('./train')\n",
        "mask_names = os.listdir('./train_masks')\n",
        "\n",
        "image_names.sort()\n",
        "mask_names.sort()\n",
        "\n",
        "# To reduce the computational time, we consider only a subset of the dataset\n",
        "N = 2000 # Number of total datapoints\n",
        "\n",
        "image_names = image_names[:N]\n",
        "mask_names = mask_names[:N]\n",
        "\n",
        "# Create data. We will always use the notation that \"x\" is the input of the \n",
        "# network, \"y\" is the output.\n",
        "x = load_data_from_names('./train', image_names)\n",
        "y = load_data_from_names('./train_masks', mask_names)\n",
        "\n",
        "# Print the dimension of the dataset.\n",
        "print(f\"The dimension of the dataset is: {x.shape}\")"
      ],
      "metadata": {
        "id": "WXr2S11DgcWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into training and test.\n",
        "TRAIN_SIZE = 1800\n",
        "\n",
        "x_train = x[:TRAIN_SIZE]\n",
        "y_train = y[:TRAIN_SIZE]\n",
        "\n",
        "x_test = x[TRAIN_SIZE:]\n",
        "y_test = y[TRAIN_SIZE:]\n",
        "\n",
        "print(f\"Train size: {x_train.shape}. Test size: {x_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDBjLSmywQwE",
        "outputId": "0e6d47b6-429b-4741-aaa8-529445ffab57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (1800, 256, 256, 3). Test size: (200, 256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize some image and the corresponding mask."
      ],
      "metadata": {
        "id": "g1WHS55kwscn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show(x, y, title=None):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(x)\n",
        "    if title:\n",
        "        plt.title(title[0])\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(y)\n",
        "    if title:\n",
        "        plt.title(title[1])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "show(x_train[0], y_train[0])"
      ],
      "metadata": {
        "id": "fndlfgdGwwtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "zrwRchzTx4V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn(input_shape, n_ch, L=3):\n",
        "    x = ks.layers.Input(shape=input_shape)\n",
        "\n",
        "    h = x\n",
        "    for i in range(L):\n",
        "        h = ks.layers.Conv2D(n_ch, kernel_size=3, padding='same')(h)\n",
        "        h = ks.layers.ReLU()(h)\n",
        "\n",
        "        n_ch = 2*n_ch\n",
        "    \n",
        "    y = ks.layers.Conv2D(1, kernel_size=1, activation='sigmoid')(h)\n",
        "    return ks.models.Model(x, y)\n",
        "\n",
        "CNN_model = build_cnn((256, 256, 3), 64, L=3)\n",
        "CNN_model.compile(optimizer=ks.optimizers.Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(CNN_model.summary())"
      ],
      "metadata": {
        "id": "ZmaEVDhQx36d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "N_EPOCHS = 20\n",
        "\n",
        "# Training\n",
        "hist = CNN_model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=N_EPOCHS,\n",
        "              validation_split=0.1)\n",
        "\n",
        "# Check overfit\n",
        "loss_history = hist.history['loss']\n",
        "val_loss_history = hist.history['val_loss']\n",
        "\n",
        "acc_history = hist.history['accuracy']\n",
        "val_acc_history = hist.history['val_accuracy']\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss', 'Val Loss'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Val Accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NBdbzsti3MdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "There are multiple metrics used to measure the quality of the segmentation. The most important are:\n",
        "\n",
        "* Accuracy\n",
        "* Intersection over Union (IoU)\n",
        "* Dice Coefficient\n",
        "\n",
        "### Accuracy\n",
        "The accuracy is simply defined by considering the segmentation as a pixel-by-pixel classification. \\\\\n",
        "\n",
        "### Intersection over Union\n",
        "The Intersection over Union (IoU) is pretty intuitive. It is defined as the ratio between the intersection area between the predicted mask and the ground truth mask, over the union between the two masks. \n",
        "\n",
        "![](https://miro.medium.com/max/300/0*kraYHnYpoJOhaMzq.png)\n",
        "\n",
        "By using that the mask is a binary image, it is trivial to compute both the intersection and the union (the latter, computed via the relationship:\n",
        "\n",
        "$$\n",
        "\\mu (A \\cup B) + \\mu (A \\cap B) = \\mu (A) + \\mu (B)\n",
        "$$\n",
        "\n",
        "where $\\mu(A)$ is defined to be the Area of A. \\\\\n",
        "\n",
        "Clearly, $IoU(y, y') \\in [0, 1]$, and $IoU(y, y') = 1$ in the best case, where $y$ and $y'$ overlap perfectly, and $IoU(y, y') = 0$ when they don't overlap.\n",
        "\n",
        "### Dice coefficient\n",
        "The Dice coefficient is defined by twice the overlapping area of the two masks, over the sum of the area of the two masks.\n",
        "\n",
        "![](https://miro.medium.com/max/429/1*yUd5ckecHjWZf6hGrdlwzA.png)\n",
        "\n",
        "The implementation of the dice coefficient is similar to the implementation of the IoU, since both of them explicitely uses that a mask is between 0 and 1."
      ],
      "metadata": {
        "id": "lCACOdNBfUwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "def iou_coeff(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "    union = K.sum(y_true,[1,2,3]) + K.sum(y_pred,[1,2,3]) - intersection\n",
        "\n",
        "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "    return iou\n",
        "\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "# Function that evaluate the model on a dataset\n",
        "def evaluate_model(model, x, y, fun):\n",
        "    y_pred = model.predict(x) # Use the model to predict the output\n",
        "    y = np.expand_dims(y, -1) # We need to add the channel dimension on y\n",
        "\n",
        "    # Uniform the type of the array\n",
        "    y_pred = y_pred.astype('float32')\n",
        "    y = y.astype('float32')\n",
        "\n",
        "    return fun(y, y_pred)\n",
        "\n",
        "iou = evaluate_model(CNN_model, x_test,  y_test, iou_coeff)\n",
        "dice = evaluate_model(CNN_model, x_test,  y_test, dice_coeff)\n",
        "\n",
        "print(f\"The IoU of the trained model is {iou}, while its Dice coefficient is {dice}.\")"
      ],
      "metadata": {
        "id": "QcwsAHvpkeFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualitative results\n",
        "y_pred = CNN_model.predict(x_test[:1])\n",
        "\n",
        "show(x_test[0, :, :], y_test[0, :, :], title='Original')\n",
        "show(x_test[0, :, :], y_pred[0, :, :, 0], title='Predicted')"
      ],
      "metadata": {
        "id": "fi_MTEshuPai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet\n",
        "\n",
        "Maybe the most known network architecture used for segmentation is the UNet. \n",
        "\n",
        "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "\n",
        "its architecture is based on the formula that you studied to compute the receptive field of a convolutional neural network:\n",
        "\n",
        "$$\n",
        "D' = S(D-1) + K\n",
        "$$\n",
        "\n",
        "where $D'$ is the receptive field of the previous layer, $D$ is the receptive field on the following layer, $S$ is the stride and $K$ is the kernel size. \\\\\n",
        "\n",
        "A consequence of this formula is that the receptive field increases exponentially while moving down, linearly while moving right. \\\\\n",
        "\n",
        "The drawback of downsampling, which is the information loss, is solved by UNet by adding skip connections, that also act as training stabilizer. \\\\\n",
        "\n",
        "Note that, at every downsampling (which in this case is implemented as a MaxPooling2D layer), the number of filters double, to reduce the impact of the dimensionality loss (the total number of pixel after downsampling is divided by 4)."
      ],
      "metadata": {
        "id": "_CD_wFRoosRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: try to implement your version of UNet, based on the diagram above.\n",
        "#           remember that the initial number of channels must be given as \n",
        "#           input and it doubles every time you go down.\n",
        "#           Moreover, it is required that the number of \"floors\" of the network\n",
        "#           must be a parameter. The same should happen for the number of each \n",
        "#           convolution per level.\n",
        "#\n",
        "#           Remember that, to go down, you need to use the MaxPooling2D operator\n",
        "#           while to go up, a strided transpose convolution is required.\n",
        "#\n",
        "#\n",
        "#\n",
        "#           Train the UNet defined in this way on the dataset above. It performs\n",
        "#           better or worse?"
      ],
      "metadata": {
        "id": "Oq0uhnlKpl57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "BATCH_SIZE = 16\n",
        "N_EPOCHS = 20\n",
        "\n",
        "hist = unet_model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=N_EPOCHS,\n",
        "              validation_split=0.1)"
      ],
      "metadata": {
        "id": "mf-N3Sr3mTfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check overfit\n",
        "loss_history = hist.history['loss']\n",
        "val_loss_history = hist.history['val_loss']\n",
        "\n",
        "acc_history = hist.history['accuracy']\n",
        "val_acc_history = hist.history['val_accuracy']\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss', 'Val Loss'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Val Accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OIzC7vGCsEE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_model.save_weights('segmentation_unet_train.h5')"
      ],
      "metadata": {
        "id": "oAuc9OagsTMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iou = evaluate_model(unet_model, x_test,  y_test, iou_coeff)\n",
        "dice = evaluate_model(unet_model, x_test,  y_test, dice_coeff)\n",
        "\n",
        "print(f\"The IoU of the trained model is {iou}, while its Dice coefficient is {dice}.\")"
      ],
      "metadata": {
        "id": "6QJuPi33tWJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualitative results\n",
        "y_pred = unet_model.predict(x_test[:1])\n",
        "\n",
        "show(x_test[0, :, :], y_test[0, :, :], title='Original')\n",
        "show(x_test[0, :, :], y_pred[0, :, :, 0], title='Predicted')"
      ],
      "metadata": {
        "id": "ODEy1-UstdzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "\n",
        "When using a very powerful model (such as UNet) to solve a Segmentation task, there is a chance that your model will overfit. Given a fixed model, overfit happens strongly if the number of datapoints is small. Thus, a way to reduce overfitting is to enlarge the dimension of the dataset. \\\\\n",
        "\n",
        "This is very easy to do in Segmentation. Given a (sufficiently regular) tranformation $T: \\mathbb{R}^{m \\times n \\times c} \\to \\mathbb{R}^{m \\times n \\times c}$ and a dataset $\\mathbb{D} = \\{ (x_i, y_i) \\}_{i=1}^N$, where $x_i$ is an image and $y_i$ is a mask with the same dimension of the corresponding image, we can create a new dataset $\\mathbb{D}_{ex} = \\{ (\\tilde{x}_i, \\tilde{y}_i) \\}_{i=1}^{2N}$ where $\\tilde{x}_i = x_i, \\tilde{y}_i = y_i$ for $i = 1, \\dots, N$, $\\tilde{x}_i = T(x_i), \\tilde{y}_i = T(y_i)$ for $i = N+1, \\dots, 2N$. \\\\\n",
        "\n",
        "This procedure can be repeated for multiple transormations $T$, to obtain a larger dataset. \\\\\n",
        "\n",
        "*Warning:* Data augmentation via regular transformations $T$ does not add information to the dataset. As a consequence, the training complexity will increase linearly with the number of transformation we apply, without resulting in an improvement on the knowledge the model has. As a consequence, simply using tens of transormations to create a big dataset is not necessary a good idea. I suggest to use at most a couple of them, such as flipping, rotating or shifting."
      ],
      "metadata": {
        "id": "mAbpUgA21MCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transform(x_data, y_data, T):\n",
        "    N = x_data.shape[0]\n",
        "\n",
        "    new_x_data = np.zeros_like(x_data)\n",
        "    new_y_data = np.zeros_like(y_data)\n",
        "\n",
        "    for i in range(N):\n",
        "        new_x_data[i] = T(x_data[i])\n",
        "        new_y_data[i] = T(y_data[i])\n",
        "    return new_x_data, new_y_data\n",
        "\n",
        "def augment_data(x_data: np.array, y_data: np.array, transforms: list) -> np.array:\n",
        "    new_x_data = x_data\n",
        "    new_y_data = y_data\n",
        "\n",
        "    for T in transforms:\n",
        "        new_x, new_y = apply_transform(x_data, y_data, T)\n",
        "\n",
        "        new_x_data = np.concatenate([new_x_data, new_x], axis=0)\n",
        "        new_y_data = np.concatenate([new_y_data, new_y], axis=0)\n",
        "    return new_x_data, new_y_data\n",
        "\n",
        "# Augment our data with flipud and fliplr\n",
        "transforms = [np.flipud, np.fliplr]\n",
        "augmented_x_train, augmented_y_train = augment_data(x_train, y_train, transforms)"
      ],
      "metadata": {
        "id": "-_Zxeym9votc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
